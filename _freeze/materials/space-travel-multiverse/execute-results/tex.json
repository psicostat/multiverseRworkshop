{
  "hash": "29db3e1cfc08e1ab3dbc9fb79cda1d0d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A space travel into the Multiverse\"\nsubtitle: \"Cognitive Science Arena\"\nengine: knitr\nbibliography: \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\nlink-citations: true\ndate: 17/02/2024\nformat:\n  minimal-beamer:\n    include-in-header:\n        - text: |\n            \\setbeamercovered{transparent}\n    # incremental: false\n---\n\n\n\n\n\n\n# Exploratory Multiverse Analysis (EMA)\n\n## An example, Statistics and Math Anxiety\n\n@McCaughey2022-tb explored the relationship between self-efficacy anxiety sensitivity and perfectionism would be related to math/statistics anxiety controlling for gender, university program, and education level.\n\nWe used the dataset available at [https://osf.io/nzhq6](https://osf.io/nzhq6/?view_only=).\n\n**We are going to do crazy stuff with this dataset that are not related to the original paper and research question! :)**\n\n## The big picture\n\n![](img/multi-big-picture.pdf)\n\n## Importing\n\nWe did a little bit of pre-processing. The `ms_anxiety.rds` file contains the cleaned version of the original dataset.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- readRDS(here(\"data/ms_anxiety.rds\"))\nvars <- names(dat)\nys <- vars[grepl(\"^stat.anx|^math\", vars)]\nys\n##  [1] \"stat.anx.tc\"   \"stat.anx.i\"    \"stat.anx.ah\"   \"stat.anx.ws\"  \n##  [5] \"stat.anx.fst\"  \"stat.anx.sc\"   \"math.anx\"      \"stat.anx.TOT\" \n##  [9] \"stat.anx.ANX\"  \"stat.anx.FEEL\"\n\nxs <- vars[!vars %in% ys]\nxs\n## [1] \"self.efficacy\"   \"asi\"             \"frost.com\"      \n## [4] \"frost.da\"        \"faculty\"         \"program.type\"   \n## [7] \"gender.category\"\n```\n:::\n\n \\normalsize\n\n\n\n\n## Exploring\n\nLet's see the type of variables of the dataset:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsapply(dat[ys], class)\n##   stat.anx.tc    stat.anx.i   stat.anx.ah   stat.anx.ws  stat.anx.fst \n##     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n##   stat.anx.sc      math.anx  stat.anx.TOT  stat.anx.ANX stat.anx.FEEL \n##     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\"\nsapply(dat[xs], class)\n##   self.efficacy             asi       frost.com        frost.da \n##       \"numeric\"       \"numeric\"       \"numeric\"       \"numeric\" \n##         faculty    program.type gender.category \n##        \"factor\"        \"factor\"        \"factor\"\n```\n:::\n\n \\normalsize\n\n\n\n\n## Main research questions\n\nThe main idea of the authors is predicting **math** and **statistics** anxiety with self-efficacy and perfectionism. In particular they pre-registered (see <https://osf.io/b3g7s>):\n\n1.  self-efficacy will be negatively related to math/statistics anxiety\n2.  anxiety sensitivity will be positively related to math/statistics anxiety.\n3.  self-critical perfectionism will be positively related to math/statistics anxiety.\n4.  the relationships described above will remain when statistically adjusting for gender, university program (arts vs. science) and student status (undergraduate vs. graduate).\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-6-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-7-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-9-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-10-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-11-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-12-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploring the relationships\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-13-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Selecting a sub-sample\n\nFor the purpose of the example, we select a subsample of the dataset to increase the variability and simulate a more uncertain scenario with a lower sample size.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(9386)\nN <- 200\nselected <- sample(1:nrow(dat), size = N, replace = FALSE)\ndat <- dat[selected, ]\n```\n:::\n\n \\normalsize\n\n\n\n\n## Data structure for specifications\n\nWhen conducting a multiverse in R (or in whatever language) the data structure is very important.\n\n-   how to create and organize the different models?\n-   how to easily extract all the informations such as coefficients, standard errors, p-values, etc.\n-   ...\n\n## An R `list` is probably the best\n\nA (named) `list` is flexible, easy to index and can be accesed by other functions to extract information and create other list.\n\nA `list` in R can be easily transformed into a `data.frame` for other models, plots, tables, etc.\n\nYou can use the `*apply` family (`sapply`, `lapply`, etc.) to compute complex operations on lists.\n\n## An R `list` is probably the best\n\nFor example, assuming that I have some regression models within a named list:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit1 <- lm(math.anx ~ gender.category + asi, data = dat)\nfit2 <- lm(math.anx ~ gender.category + asi + faculty, data = dat)\nfit3 <- lm(math.anx ~ gender.category + faculty, data = dat)\nfit4 <- lm(math.anx ~ gender.category + asi + program.type, data = dat)\n\n# ... and other thousands of (plausible) models :)\n\nmods <- list(fit1, fit2, fit3, fit4)\nnames(mods) <- paste0(\"mod\", 1:length(mods))\n```\n:::\n\n \\normalsize\n\n\n\n\n## An R `list` is probably the best\n\nThen, I want to extract all the `asi` coefficients and put into a data.frame:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_coef <- function(x, coef = NULL){\n  x <- broom::tidy(x, conf.int = TRUE)\n  if(!is.null(coef)){\n    filter(x, term %in% coef)\n  } else{\n    x\n  }\n}\n\nget_coef(mods$mod1, \"asi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 7\n  term  estimate std.error statistic     p.value conf.low conf.high\n  <chr>    <dbl>     <dbl>     <dbl>       <dbl>    <dbl>     <dbl>\n1 asi      0.383    0.0748      5.11 0.000000745    0.235     0.530\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## An R `list` is probably the best\n\nWith `lapply` (or `purrr::map()`) and combining the results, you can easily create a nice dataframe with your coefficients:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlapply(mods, get_coef, \"asi\") |> \n  dplyr::bind_rows(.id = \"mod\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 8\n  mod   term  estimate std.error statistic  p.value conf.low conf.high\n  <chr> <chr>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 mod1  asi      0.383    0.0748      5.11  7.45e-7    0.235     0.530\n2 mod2  asi      0.386    0.0741      5.21  4.88e-7    0.240     0.532\n3 mod4  asi      0.410    0.0728      5.64  5.89e-8    0.267     0.554\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Creating the specifications\n\nThere are multiple ways of creating the specifications in practice. You can do it from scratch:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod1 <- lm(y ~ x1 + x2)\nmod2 <- lm(y ~ log(x1) + log(x2))\nmod3 <- lm(y ~ x1 + x2) # removing outliers\n\nmods <- list(mod1 = mod1, mod2 = mod2, mod3 = mod3)\n# ...\n```\n:::\n\n \\normalsize\n\n\n\n\n## Creating the specifications\n\nThe [`multiverse`](https://cran.r-project.org/web/packages/multiverse/readme/README.html) R Package and the related paper [@Gotz2024-ov] provides a very flexible and complex syntax to define different specifications.\n\n## Creating the specifications\n\nFor this example we can use some custom functions, in particular the `create_multi()` function. There are no wrong solutions if the results is correct.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndevtools::load_all()\n\nslog <- function(x) {\n  if(any(x == 0)){\n    x <- x + 1\n  }\n  log(x)\n}\n\nmulti <- create_multi(\n  math.anx ~ asi + faculty + stat.anx.TOT, # full model formula\n  focal = \"asi\", # focal predictor, never removed\n  nfuns = c(\"slog\"), # functions for the numeric variables\n  data = dat\n)\n```\n:::\n\n \\normalsize\n\n\n\n\n## Creating the specifications\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n$X\n       fun            x    type focal .id_fun .id_x\n1 identity          asi numeric  TRUE       1     1\n2 identity      faculty  factor FALSE       1     2\n3 identity stat.anx.TOT numeric FALSE       1     3\n5     slog stat.anx.TOT numeric FALSE       2     3\n                call\n1                asi\n2            faculty\n3       stat.anx.TOT\n5 slog(stat.anx.TOT)\n\n$calls\n[1] \"~ asi\"                               \n[2] \"~ asi + faculty\"                     \n[3] \"~ asi + stat.anx.TOT\"                \n[4] \"~ asi + slog(stat.anx.TOT)\"          \n[5] \"~ asi + faculty + stat.anx.TOT\"      \n[6] \"~ asi + faculty + slog(stat.anx.TOT)\"\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Creating the specifications\n\nWhatever the method we used, we need:\n\n-   a list of models\n-   a way to easily extract the coefficients or other quantities\n-   a way to extract a summary of the specifications i.e. if a variable is included or not, the type of tranformation, etc.\n\n## Pay attention with interactions!\n\nWhen an interaction is included in the model, the interpretation of the model coefficients completely change, especially if the interaction is consistent. You cannot compare a focal coefficients directly for models with and without interactions.\n\nLet's assume that `asi` is the focal coefficient and we include in the multiverse these two models:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_int <- lm(math.anx ~ faculty + asi + faculty:asi, data = dat)\nfit_no_int <- lm(math.anx ~ faculty + asi, data = dat)\n```\n:::\n\n \\normalsize\n\n\n\n\n## Pay attention with interactions!\n\nThe `asi` effect in one case is the overall effect (i.e., main effect) controlling for `faculty`. In the other case is the `asi` effect of the reference value.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-22-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Pay attention with interactions!\n\nOne should adjust the contrasts coding of factors and/or the centering of numeric variables.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# sum to zero contrasts i.e. estimating the main effect of asi\nupdate(fit_int, contrasts = list(faculty = contr.sum(3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = math.anx ~ faculty + asi + faculty:asi, data = dat, \n    contrasts = list(faculty = contr.sum(3)))\n\nCoefficients:\n (Intercept)      faculty1      faculty2           asi  faculty1:asi  \n      1.8597        0.6839       -0.4048        0.3087       -0.1234  \nfaculty2:asi  \n      0.1590  \n```\n\n\n:::\n\n```{.r .cell-code}\n# with emmeans\nemmeans::emtrends(fit_int, ~1, var = \"asi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 1       asi.trend   SE  df lower.CL upper.CL\n overall     0.309 0.11 194   0.0925    0.525\n\nResults are averaged over the levels of: faculty \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Why exploring is important?\n\nA multiverse analysis increase the complexity of the data analysis. **There is no longer a single dataset and result to discuss**.\n\n## Let's create some scenarios :)\n\nFirstly, we use variable transformations directly within the model formula. In this way it is easier to extract the conditions. Thus we define some wrappers:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# safe version of log() with 0 variables\nslog <- function(x){\n  if(any(x == 0)){\n    x <- x + 1\n  }\n  log(x)\n}\n\n# function factories, see https://adv-r.hadley.nz/function-factories.html\npolyN <- function(degree = 1){\n  function(x) poly(x, degree = degree)\n}\n\npoly2 <- polyN(2)\npoly3 <- polyN(3)\n```\n:::\n\n \\normalsize\n\n\n\n\n## Let's create some scenarios :)\n\nMore wrappers:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncutN <- function(breaks){\n  function(x){\n    cut(x, breaks = breaks)\n  }\n}\n\ncut2 <- cutN(2)\ncut4 <- cutN(4)\n```\n:::\n\n \\normalsize\n\n\n\n\n## Let's create some scenarios :)\n\nThen we can identify some univariate/multivariate outliers or some observations that we may consider removing for some reasons.\n\n## Let's create some scenarios :)\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfocal <- \"self.efficacy\"\n\nmulti <- create_multi(\n  math.anx ~ self.efficacy + faculty + asi + gender.category + \n      program.type + frost.da,\n  focal = focal,\n  nfuns = c(\"slog\", \"cut2\", \"poly2\"),\n  data = dat\n)\n```\n:::\n\n \\normalsize\n\n\n\n\n## Let's fit the models\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# faster than before\nget_coef <- function(x, coef = NULL){\n  xs <- data.frame(summary(x)$coefficients)\n  if(!is.null(coef)){\n    xs <- xs[coef, ]\n  }\n  xs$param <- rownames(xs)\n  return(xs)\n}\n\nfitl <- vector(mode = \"list\", length = length(multi$calls))\n\nfor(i in 1:length(multi$calls)){\n  form <- paste0(\"math.anx\", multi$calls[i])\n  fitl[[i]] <- glm(form, family = gaussian(link = \"identity\"), data = dat)\n}\n\nresl <- lapply(fitl, get_coef, focal)\nres <- bind_rows(resl, .id = \"mod\")\nrownames(res) <- NULL\nnames(res) <- c(\"mod\", \"b\", \"se\", \"t\", \"p\", \"param\")\n```\n:::\n\n \\normalsize\n\n\n\n\n## Let’s fit the models\n\nNow we have a dataframe with all the model coefficients across the specifications. We can start our multiverse!\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mod          b         se         t            p         param\n1   1 -0.3955213 0.10133460 -3.903122 0.0001301284 self.efficacy\n2   2 -0.3315150 0.10439789 -3.175495 0.0017375474 self.efficacy\n3   3 -0.3302943 0.09599808 -3.440634 0.0007084586 self.efficacy\n4   4 -0.3041665 0.10324396 -2.946095 0.0036060375 self.efficacy\n5   5 -0.3335836 0.09874191 -3.378339 0.0008788681 self.efficacy\n6   6 -0.2266493 0.10148550 -2.233317 0.0266529679 self.efficacy\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Exploratory tools\n\n-   Marginal/Conditional effects\n-   Vibration of Effects\n-   Specification Curve\n\n## Marginal/Conditional effects\n\nOverall distribution of regression parameters:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-29-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Marginal/Conditional effects\n\nWe can combine the model results with a table created by all conditions with the custom `get_info_models()` function:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninfo <- get_info_models(multi)\nhead(info)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 7\n    mod x_self.efficacy x_faculty x_asi x_gender.category\n  <int> <chr>           <chr>     <chr> <chr>            \n1     1 self.efficacy   <NA>      <NA>  <NA>             \n2     2 self.efficacy   faculty   <NA>  <NA>             \n3     3 self.efficacy   <NA>      asi   <NA>             \n4     4 self.efficacy   <NA>      <NA>  gender.category  \n5     5 self.efficacy   <NA>      <NA>  <NA>             \n6     6 self.efficacy   <NA>      <NA>  <NA>             \n# i 2 more variables: x_program.type <chr>, x_frost.da <chr>\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Marginal/Conditional effects\n\nThen we can combine the `info` table with the coefficients table and we have all the important information.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# same type\nres$mod <- as.numeric(res$mod)\ninfo$mod <- as.numeric(info$mod)\n# merging the two tables\nmulti_res <- left_join(res, info, by = \"mod\")\nhead(multi_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mod          b         se         t            p         param\n1   1 -0.3955213 0.10133460 -3.903122 0.0001301284 self.efficacy\n2   2 -0.3315150 0.10439789 -3.175495 0.0017375474 self.efficacy\n3   3 -0.3302943 0.09599808 -3.440634 0.0007084586 self.efficacy\n4   4 -0.3041665 0.10324396 -2.946095 0.0036060375 self.efficacy\n5   5 -0.3335836 0.09874191 -3.378339 0.0008788681 self.efficacy\n6   6 -0.2266493 0.10148550 -2.233317 0.0266529679 self.efficacy\n  x_self.efficacy x_faculty x_asi x_gender.category x_program.type\n1   self.efficacy      <NA>  <NA>              <NA>           <NA>\n2   self.efficacy   faculty  <NA>              <NA>           <NA>\n3   self.efficacy      <NA>   asi              <NA>           <NA>\n4   self.efficacy      <NA>  <NA>   gender.category           <NA>\n5   self.efficacy      <NA>  <NA>              <NA>   program.type\n6   self.efficacy      <NA>  <NA>              <NA>           <NA>\n  x_frost.da\n1       <NA>\n2       <NA>\n3       <NA>\n4       <NA>\n5       <NA>\n6   frost.da\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Marginal/Conditional effects\n\nFinally we can plot also the distributions of parameters conditioned on the presence/absence of a particular other predictor:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-32-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Vibration of Effects (VoE) [@Patel2015-pn]\n\n![](img/patel-vibration.pdf)\n\n## Vibration of Effects (VoE) [@Patel2015-pn]\n\nThe VoE is a statistical approach to evaluate the variability in effect estimates and p value due to different sources of variability (i.e., *vibrations*)\n\n-   **sampling** vibration: subsets of the full dataset\n-   **model** vibration: combinations of control variables\n-   **pre-processing** vibration: inclusio/exclusion criteria, outliers, etc.\n\n## Vulcano Plot\n\nThe Vulcano Plot is the graphical tool used in the VoE as a diagnostic tool.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-33-1.pdf){fig-align='center' width=85%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Vulcano Plot\n\nThe `x` axis is the effect size. Usually a regression coefficient of a *focal* parameter. Can be a raw or standardized regression coefficient or whatever effect size measure.\n\nThe `y` axis is the associated p-value transformed in $-\\log_{10}(p)$ for better intepretation and visualization. Higher tranformed p values are smaller raw p values.\n\n## Vibration of Effects (VoE)\n\nThe authors proposed to summarise the VoE using the range of effect sizes and p values. In particular the difference between the $99^{th}$ and $1^{st}$ percentiles.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-34-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Vibration of Effects (VoE)\n\nThey identified three usual pattern for a Vulcano Plot:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-35-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Vibration of Effects (VoE)\n\nThe **Robust** plot suggests a stable pattern across specifications, with the majority if not the total being positive and significant.\n\nThe **Janus**[^1] plot suggests the worst scenario where in some conditions the effect is not only not significant but reversed.\n\n[^1]: Fun fact: Janus comes from the Roman/Greek god with two faces :)\n\nThe **Mixed** plot suggests a less robust effect with few effect size reversals in rare specifications.\n\n## P-values transformation\n\nThere are different ways to transform p-values to improve the interpretation and visualization.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-36-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## P-values transformation\n\nValues higher than \\~1.3 (in $\\log_{10}$) or \\~2 ($z$ transformation) are significant assuming the traditional $\\alpha = 0.05$.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-37-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Vulcano plot with our data\n\nWe can create a basic version of the vulcano plot with our dataset:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmulti_res |> \n    mutate(sign = ifelse(p <= 0.05, \"<= 0.05\", \"> 0.05\"),\n           sign = factor(sign, levels = c(\"<= 0.05\", \"> 0.05\"))) |> \n    ggplot(aes(x = b, y = tp(p, \"-log10\"))) +\n    geom_point(aes(shape = sign, color = sign), size = 5) +\n    ylab(\"-log10(p)\") +\n    xlab(focal) +\n    scale_shape_manual(values = c(3, 16)) +\n    theme(legend.title = element_blank())\n```\n:::\n\n \\normalsize\n\n\n\n\n## Vulcano plot with our data\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-39-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Marginal/Conditional effects\n\nA way to evaluate the impact of the multiverse scenario could be to use an ANOVA-style way of thinking. We can fit a regression model on the multiverse where the focal coefficient is the response variable and a series of dummy variables to code the inclusion/exclusion of a certain predictor.\n\nThen we can estimate the % of explained variance of each predictor as an index of the impact in the multiverse results.\n\nA more refined version of this approach can be found in @Klau2023-sb\n\n## Decomposing the multiverse variance\n\nWe can create a dataset with dummy variables when a specific predictor is included or not. We are ignoring the transformations of the specific variable.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 6\n       b faculty   asi gender.category program.type frost.da\n   <dbl>   <dbl> <dbl>           <dbl>        <dbl>    <dbl>\n1 -0.396       0     0               0            0        0\n2 -0.332       1     0               0            0        0\n3 -0.330       0     1               0            0        0\n4 -0.304       0     0               1            0        0\n5 -0.334       0     0               0            1        0\n6 -0.227       0     0               0            0        1\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Decomposing the multiverse variance\n\nThen we can fit a linear regression and then evaluate the impact of including/excluding a predictor.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lm(b ~ ., data = multi_fit)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = b ~ ., data = multi_fit)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.071822 -0.013166  0.002804  0.014675  0.037509 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -0.323699   0.004734 -68.375   <2e-16 ***\nfaculty          0.027851   0.002733  10.190   <2e-16 ***\nasi              0.002285   0.003417   0.669    0.504    \ngender.category  0.037357   0.002733  13.668   <2e-16 ***\nprogram.type     0.028599   0.002733  10.463   <2e-16 ***\nfrost.da         0.099365   0.003417  29.083   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01933 on 194 degrees of freedom\nMultiple R-squared:  0.8653,\tAdjusted R-squared:  0.8618 \nF-statistic: 249.3 on 5 and 194 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Decomposing the multiverse variance\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neffectsize::eta_squared(fit, partial = FALSE)\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n\n\n## Specification Curve [@Simonsohn2020-sr]\n\nThe specification curve is both an inferential and descriptive tool to summarise the results from a multiverse analysis.\n\n![](img/spec-curve-example.pdf)\n\n## Specification Curve as descriptive tool\n\nBasically from *M* specifications we extract the focal coefficient then:\n\n-   we sort the coefficients from the lowest to the highest and assign a progressive index\n-   we plot the index as a function of the coefficient value\n-   for each scenario we code the corresponding set of conditions/variables\n-   we combine the previous plot with a tile-plot (or similar) showing for each scenario the set of variables/choices\n\n## Specification with the dataset\n\n\n\n\n\n \\tiny\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nspec_data <- multi_res |> \n    # sorting\n    arrange(desc(b)) |> \n    # index with the order\n    mutate(spec = 1:n())\n\ntop <- spec_data |> \n    # confidence intervals\n    mutate(lb = b - se * 2,\n           ub = b + se * 2) |> \n    ggplot(aes(x = spec, y = b)) +\n    geom_point() +\n    theme(axis.text.x = element_blank(),\n          axis.title.x = element_blank(),\n          axis.title.y = element_blank())\n\nbottom <- spec_data |> \n    pivot_longer(starts_with(\"x_\")) |> \n    drop_na() |> \n    mutate(name = gsub(\"x_\", \"\", name)) |> \n    ggplot(aes(x = spec, y = value)) +\n    geom_point() +\n    theme(axis.title.y = element_blank(),\n          strip.text.y = element_text(size = 9),\n          axis.text.y = element_text(size = 9)) +\n    xlab(\"Specification\") +\n    facet_grid(name~., scales = \"free\")\n```\n:::\n\n \\normalsize\n\n\n\n\n## Specification with the dataset\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-45-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Other descriptive tools\n\nIn general, any descriptive statistics can be useful. The main points in a multiverse description are:\n\n-   the range of the estimated effects\n-   the impact of the choices\n-   the impact on the conclusions (e.g., statistical significance)\n\n# Can the EMA be misleading?\n\n## Let's have a look to another example\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n\n\nWe have a multiverse with 31 scenarios, 50 observations and 5 predictors, this is the vulcano plot. **What do you think?**\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-47-1.pdf){fig-align='center' width=95%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Let's have a look to another example[^2]\n\n[^2]: Thanks to Livio Finos for the insightful example\n\nFrom the previous multiverse it is clear that something is going on. Some of the coefficients are significant and other not. There is also a little bit of Janus effect.\n\n. . .\n\nBut, the previous example was a simulated multiverse where all the coefficients $\\beta_j = 0$ (the null hypothesis is true). **All the significant scenarios are false positives (type-1 error)!**\n\n## Why? multiple testing problem!\n\n- A multiverse can be considered as a **multiple testing problem** because we are testing a set of hypotheses with the same dataset. The type-1 error rate ($\\alpha$) need to be controlled otherwise the actual level is higher than the nominal level.\n- We can demonstrate this with a simple simulation. We simulate $k$ variables and a one-sample t-test for each variable. The ground truth is that we have $\\mu_1, \\mu_2, \\dots, \\mu_k = 0$ thus $H_0$ is true.\n- We repeat the simulation $B$ times and we count how many times $p \\leq \\alpha$ for at least one of the $k$ tests. This is our estimated type-1 error rate.\n\n## Why? multiple testing problem!\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10  # number of variables\nn <- 100 # number of observations\nR <- 0 + diag(1 - 0, k) # correlation matrix\nB <- 1e3\n\nPM <- matrix(NA, B, k)\n\nfor(i in 1:B){\n    X <- MASS::mvrnorm(n, rep(0, k), R)\n    p <- apply(X, 2, function(x) t.test(x)$p.value)\n    PM[i, ] <- p\n}\n\n# type-1 error for each variable, ignoring multiple testing\napply(PM, 2, function(x) mean(x <= 0.05))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.048 0.045 0.055 0.052 0.052 0.061 0.049 0.049 0.056 0.059\n```\n\n\n:::\n\n```{.r .cell-code}\n# type-1 error considering the k tests (should be alpha)\nmean(apply(PM, 1, function(x) any(x <= 0.05)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.427\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Why? multiple testing problem!\n\nTo have a better overview, we can repeat the simulation for different number of $k$. Quite scary right?\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-50-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n\n## So what? No multiverse?\n\nExploring is fine and is quite important if not fundamental. But, when we explore the p-values thus the **inferential** results from the single scenarios, we are inflating the type-1 error and our **inferential** conclusions are no longer valid.\n\n. . .\n\nIf we want an inferential answer from our multiverse (not always the case) we need a proper inferential framework. This is the role of the **inferential multiverse analaysis**.\n\n# Inferential Multiverse Analysis (IMA)\n\n## Family-wise error rate (FWER)[^3]\n\n[^3]: Thanks to Anna Vesely for the amazing introduction to the multiple testing problem (see the [slides](https://psicostat.dpss.psy.unipd.it/files/2023-04-28_vesely.pdf))\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\\begin{table}\n\\centering\\begingroup\\fontsize{8}{10}\\selectfont\n\n\\begin{tabular}{>{}ccccc}\n\\toprule\n\\multicolumn{2}{c}{\\textbf{ }} & \\multicolumn{2}{c}{\\textbf{$H_0$}} & \\multicolumn{1}{c}{\\textbf{ }} \\\\\n\\cmidrule(l{3pt}r{3pt}){3-4}\n\\textbf{} & \\textbf{} & \\textbf{False} & \\textbf{True} & \\textbf{Tot}\\\\\n\\midrule\n & Rejected & \\textcolor[HTML]{2cb600}{\\textbf{True Positive (S)}} & \\textcolor[HTML]{b22222}{\\textbf{False Positive (V)}} & $R$\\\\\n\\cmidrule{2-5}\n\\multirow{-2}{*}{\\centering\\arraybackslash \\textbf{Test}} & Not rejected & \\textcolor[HTML]{000000}{\\textbf{False Negative (T)}} & \\textcolor[HTML]{000000}{\\textbf{True Negative (U)}} & $m - R$\\\\\n\\cmidrule{1-5}\n\\textbf{} & \\textbf{Tot} & \\textcolor[HTML]{000000}{\\textbf{$m_1$}} & \\textcolor[HTML]{000000}{\\textbf{$m_0$}} & $m$\\\\\n\\bottomrule\n\\end{tabular}\n\\endgroup{}\n\\end{table}\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\nThe FWER is the probability of committing type-1 error thus $\\mbox{P}(V > 0)$. Controlling the FWER (whatever the methods) keep $\\mbox{P} \\leq \\alpha$.\n\nThere are different procedures for controlling the FWER, such as the Bonferroni or the Holm–Bonferroni method.\n\n## Correcting the p-values\n\nThe main problem is that the number of tests in a multiverse can be quite large.\n\n. . .\n\nAs an example, we simulated a series of tests with different effect size to show the impact on the type-1 error rate and the power.\n\n## Correcting the p-values\n\nUsing a standard method (e.g., Bonferroni) clearly controls the type-1 error but reduces a lot the statistical power. At the same time, without correction the inflation is large.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-53-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Correlation between scenarios is (probably) large\n\nThe multiverse scenarios are computed on the same dataset thus the correlation between tests is probably medium-large. For example:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- runif(100, 5, 10)\ny <- x * 0.1 + rnorm(100)\n\nfit1 <- lm(y ~ x)\nfit2 <- lm(y ~ cut(x, breaks = 2))\nfit3 <- lm(y ~ log(x))\nfit4 <- lm(y ~ poly(x, 2))\n\npp <- sapply(list(fit1, fit2, fit3, fit4), predict)\nround(cor(pp), 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.88 1.00 0.99\n[2,] 0.88 1.00 0.88 0.88\n[3,] 1.00 0.88 1.00 1.00\n[4,] 0.99 0.88 1.00 1.00\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## A more powerful correction method[^4]\n\n[^4]: @Goeman2014-om\n\nThe Bonferroni (and similar) methods assume that the tests are independent thus regardless the dependence structure the FWER is under control.\n\nThe permutation-based methods (maxT, minP, etc.) take into account the correlation structure providing FWER control under $H_0$ but a more powerful test under $H_1$.\n\n## Permutation testing in a nutshell\n\nPermutation testing requires computing the distribution of the test statistics $T$ where we know that $H_0$ is true.\n\n. . .\n\nWe can force the null to be true permuting the data *removing* the assumed effect. We repeat this process a large number of times $B$.\n\n. . .\n\nThen we compare the observed test statistics $T_1$ with the distribution of permuted test statistics obtaining the permutation based p-value $p = \\frac{\\#(T_1 \\geq \\mathbf{T}_B)}{B}$[^5]\n\n[^5]: $\\#$ is the count function.\n\n## Permutation testing in a nutshell\n\nLet's make an example with a two-groups comparison:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- 30\nd <- 1 # effect size\nx <- rep(c(0, 1), each = N/2) # dummy for the group\ny <- rnorm(N, d * x, 1)\ntapply(y, x, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         0          1 \n-0.3933720  0.4743739 \n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Permutation testing in a nutshell\n\nLet's make an example with a two-groups comparison:\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-56-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Permutation testing in a nutshell\n\nWe need to flip the group label thus removing the group effect.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- 1e3 # number of permutations\ntp <- rep(NA, B)\ntp[1] <- t.test(y ~ x)$statistic # first permutation always the observed data\n\nsample(x) # shuffling the group label\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1\n```\n\n\n:::\n\n```{.r .cell-code}\nfor(i in 2:B){\n    xp <- sample(x)\n    tp[i] <- unname(t.test(y ~ xp)$statistic)\n}\n\nmean(abs(tp) >= abs(tp[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.038\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Permutation testing in a nutshell\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-58-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## maxT procedure @Westfall1993-ek\n\nThe maxT is a permutation-based method to control the FWER. With the method we can obtain:\n\n-   overall inference across $M$ tests with *weak* control of FWER\n-   individually adjusted p-values for each test (i.e, *strong* FWER control)\n\n## maxT with correlated variables\n\nBeyond the actual method and algorithm, the advantage of the maxT approach is taking into account the correlation between tests.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-59-1.pdf){fig-align='center' width=85%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Inferential Methods\n\n-   Specification Curve\n-   Post-Selection Inference in Multiverse Analysis (PIMA)\n\n## Specification Curve\n\nThe specification curve [@Simonsohn2020-sr] is the first attempt to build an inferential framework for multiverse analysis.\n\n-   provides only *weak* control of type-1 error\n-   is not directly applicable to GLMs [only standard linear models, see @Girardi2024-ip]\n-   is computationally expensive\n\n## The PIMA recipe... [@Girardi2024-ip]\n\nPIMA provides *weak* and *strong* type-1 error control with a powerful method based on permutations (maxT) and applicable to whatever GLM (Logistic, Poisson, etc.).\n\nFor constructing the inferential approach with PIMA we need:\n\n-   a flexible modelling framework: **Generalized Linear Models**\n-   a permutation-based inferential approach: **Flipscores**\n-   a permutation-based and powerful method for weak and strong FWER control: **maxT**\n\n## The core of PIMA, the **flipscores** method\n\n- The formal part of the **flipscores** method is quite complex and beyond our scope and expertise. But a detailed description can be found in @Hemerik2020-nj and @Girardi2024-ip.\n- Essentially the **flipscores** method is an alternative way of doing inference for parameters of a GLM based on permutations.\n- The idea is conceptually the same as the two-groups example, but can works for multiple regression models with covariates and interactions.\n\n## Intution of **flipscores**\n\nThis method can be extended to whatever GLM and to any number of predictors/confounders.\n\nThe actual permutation test is obtained flipping the sign of the scores/residuals thus obtaining the distribution under the null hypothesis of the test statistics.\n\nEverthing is implemented into the `flipscores` package [@Hemerik2020-nj] and on CRAN <https://cran.r-project.org/web/packages/flipscores/index.html>.\n\n## `flipscores` package\n\nWith the `flipscores` function is very easy to fit a linear model with permutations-based p-values.\n\n\n\n\n\n \\tiny\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(flipscores)\nfit <- flipscores(Sepal.Length ~ Petal.Width + Species, data = iris)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nflipscores(formula = Sepal.Length ~ Petal.Width + Species, data = iris)\n\nCoefficients:\n                   Estimate     Score Std. Error   z value Part. Cor\n(Intercept)         4.78044 160.25913   13.50622  11.86558     0.979\nPetal.Width         0.91690   5.64500    1.27732   4.41941     0.365\nSpeciesversicolor  -0.06025  -0.26260    1.00098  -0.26234    -0.022\nSpeciesvirginica   -0.05009  -0.09030    0.64372  -0.14028    -0.012\n                  Pr(>|z|)    \n(Intercept)         0.0002 ***\nPetal.Width         0.0002 ***\nSpeciesversicolor   0.7932    \nSpeciesvirginica    0.9008    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2313718)\n\n    Null deviance: 102.17  on 149  degrees of freedom\nResidual deviance:  33.78  on 146  degrees of freedom\nAIC: 212.07\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Intuition of PIMA\n\nThe idea of PIMA is to extend the `flipscores` method to $M$ models (where $M$ is the number of scenarios) and perform inference at the multiverse level.\n\nUsing the maxT approach we can combine the $M$ tests into a single test with weak control of FWER. The global null hypothesis is:\n\n$$\n\\mathcal{H} = \\bigcap_{m=1}^{M} \\mathcal{H}_m : \\beta_m = 0 \\ \\text{for all} \\ m = 1, \\ldots, M.\n$$\n\nIn addition, we can correct the indidual p-values with strong FWER control using the maxT method.\n\n## The `pima` package\n\nWe are implementing everything into the `pima` package that is under development. You can try it but there could be bugs and breaking changes in the near future.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n\n\nYou can explore the package here <https://github.com/livioivil/pima>. The package mainly depends on `jointest` that is the actual package for combining multiple (correlated) tests and correcting them.\n\n## The `pima` package\n\nThe package has a main function called `pima::pima()` that takes a list of models (of class `glm`) and compute the global test and the correction for individual scenarios.\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fitl is the list of fitted models\n# tested_coeffs is the focal variable, other are confounders\n\nlibrary(pima)\n\n# this is a bug/part to be improved, ignore\nfor(i in 1:length(fitl)){\n    fitl[[i]]$call$formula <- as.formula(fitl[[i]]$formula)\n}\n\nmulti_pima <- pima(fitl, tested_coeffs = \"self.efficacy\")\n```\n:::\n\n \\normalsize\n\n\n\n\n## The `pima` results\n\nThe correlations between the scenarios is very high, the maxT method will be powerful!\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-63-1.pdf){fig-align='center' width=90%}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## The `pima` results\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# overall test, weak control FWER\nsummary(pima::global_tests(multi_pima))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Model         Coeff Stat nTests     S     p\n1 Overall self.efficacy maxT    200 30.26 6e-04\n```\n\n\n:::\n\n```{.r .cell-code}\n# adjusted p values, strong control FWER\nhead(summary(multi_pima))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Model .assign         Coeff   Estimate     Score Std. Error\n1 Model1       1 self.efficacy -0.3955213 -30.26473   8.026511\n2 Model2       1 self.efficacy -0.3315150 -23.30487   7.506267\n3 Model3       1 self.efficacy -0.3302943 -24.84790   7.416916\n4 Model4       1 self.efficacy -0.3041665 -21.44863   7.420203\n5 Model5       1 self.efficacy -0.3335836 -24.92025   7.567967\n6 Model6       1 self.efficacy -0.2266493 -15.42204   6.974630\n    z value  Part. Cor      p p.adj.maxT\n1 -3.770596 -0.2672905 0.0004      0.001\n2 -3.104722 -0.2212023 0.0020      0.005\n3 -3.350167 -0.2380860 0.0014      0.004\n4 -2.890572 -0.2054240 0.0020      0.010\n5 -3.292859 -0.2340133 0.0020      0.004\n6 -2.211162 -0.1571404 0.0240      0.072\n```\n\n\n:::\n:::\n\n \\normalsize\n\n\n\n\n## maxT correction\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-65-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## Improved vulcano plot\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](space-travel-multiverse_files/figure-beamer/unnamed-chunk-66-1.pdf){fig-align='center'}\n:::\n:::\n\n \\normalsize\n\n\n\n\n## References {.allowframebreaks}\n\n\\scriptsize\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}